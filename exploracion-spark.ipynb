{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploracion del dataset de estaciones de Brasil usando PySpark\n",
    "\n",
    "fuentes:\n",
    "- https://sparkbyexamples.com/pyspark/pyspark-read-csv-file-into-dataframe/\n",
    "- https://sparkbyexamples.com/pyspark/pyspark-where-filter/\n",
    "- https://sparkbyexamples.com/pyspark/pyspark-dataframe-groupby-and-sort-by-descending-order/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\apps\\\\spark-3.5.0-bin-hadoop3'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inicializamos una sesion de spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Perla Negra\") \\\n",
    "                            .master(\"local[*]\") \\\n",
    "                            .config(\"spark.ui.port\", \"4040\") \\\n",
    "                            .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Para leer un archivo CSV en un DataFrame de PySpark, usa `csv(\"path\")` de DataFrameReader. Este artículo explora el proceso de lectura de archivos individuales, múltiples archivos o todos los archivos de un directorio local en un DataFrame utilizando PySpark.\n",
    "\n",
    "Puntos clave:\n",
    "\n",
    "- PySpark admite la lectura de archivos CSV con pipes, coma, tab, espacio u otros delimitadores/separadores.\n",
    "- PySpark lee archivos CSV en paralelo, aprovechando múltiples nodos ejecutores para acelerar la ingestión de datos.\n",
    "- PySpark puede inferir automáticamente el esquema de los archivos CSV, eliminando la necesidad de definir el esquema manualmente en muchos casos.\n",
    "- Los usuarios tienen la flexibilidad de definir esquemas personalizados para los archivos CSV, especificando tipos de datos y nombres de columnas según sea necesario.\n",
    "- PySpark ofrece opciones para manejar encabezados en los archivos CSV, permitiendo a los usuarios omitir los encabezados o tratarlos como filas de datos.\n",
    "- Proporciona mecanismos robustos de manejo de errores para tratar con archivos CSV mal formateados o corruptos, asegurando la integridad de los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      " |-- _c4: string (nullable = true)\n",
      " |-- _c5: string (nullable = true)\n",
      " |-- _c6: string (nullable = true)\n",
      " |-- _c7: string (nullable = true)\n",
      " |-- _c8: string (nullable = true)\n",
      " |-- _c9: string (nullable = true)\n",
      " |-- _c10: string (nullable = true)\n",
      " |-- _c11: string (nullable = true)\n",
      " |-- _c12: string (nullable = true)\n",
      " |-- _c13: string (nullable = true)\n",
      " |-- _c14: string (nullable = true)\n",
      " |-- _c15: string (nullable = true)\n",
      " |-- _c16: string (nullable = true)\n",
      " |-- _c17: string (nullable = true)\n",
      " |-- _c18: string (nullable = true)\n",
      " |-- _c19: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Read CSV File\n",
    "df = spark.read.option(\"delimiter\", \";\").csv(\"datasets/brazil-weather/conventional_weather_stations_inmet_brazil_1961_2019.csv\", )\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqui se leen los datos de las estaciones de brazil dentro de un DataFrame de Spark. _c0, _c1, etcetera son las columnas leidas en el dataset. Por defecto las columnas se tratan como strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      " |-- _c4: string (nullable = true)\n",
      " |-- _c5: string (nullable = true)\n",
      " |-- _c6: string (nullable = true)\n",
      " |-- _c7: string (nullable = true)\n",
      " |-- _c8: string (nullable = true)\n",
      " |-- _c9: string (nullable = true)\n",
      " |-- _c10: string (nullable = true)\n",
      " |-- _c11: string (nullable = true)\n",
      " |-- _c12: string (nullable = true)\n",
      " |-- _c13: string (nullable = true)\n",
      " |-- _c14: string (nullable = true)\n",
      " |-- _c15: string (nullable = true)\n",
      " |-- _c16: string (nullable = true)\n",
      " |-- _c17: string (nullable = true)\n",
      " |-- _c18: string (nullable = true)\n",
      " |-- _c19: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# una manera alternativa de leer un archivo csv con Spark es usando format().load()\n",
    "\n",
    "df = spark.read.option(\"delimiter\", \";\").format(\"csv\") \\\n",
    "                  .load(\"datasets/brazil-weather/conventional_weather_stations_inmet_brazil_1961_2019.csv\")\n",
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si especificamos la opcion header=True, spark va a intentar encontrar los nombres de las columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Estacao: string (nullable = true)\n",
      " |-- Data: string (nullable = true)\n",
      " |-- Hora: string (nullable = true)\n",
      " |-- Precipitacao: string (nullable = true)\n",
      " |-- TempBulboSeco: string (nullable = true)\n",
      " |-- TempBulboUmido: string (nullable = true)\n",
      " |-- TempMaxima: string (nullable = true)\n",
      " |-- TempMinima: string (nullable = true)\n",
      " |-- UmidadeRelativa: string (nullable = true)\n",
      " |-- PressaoAtmEstacao: string (nullable = true)\n",
      " |-- PressaoAtmMar: string (nullable = true)\n",
      " |-- DirecaoVento: string (nullable = true)\n",
      " |-- VelocidadeVento: string (nullable = true)\n",
      " |-- Insolacao: string (nullable = true)\n",
      " |-- Nebulosidade: string (nullable = true)\n",
      " |-- Evaporacao Piche: string (nullable = true)\n",
      " |-- Temp Comp Media: string (nullable = true)\n",
      " |-- Umidade Relativa Media: string (nullable = true)\n",
      " |-- Velocidade do Vento Media: string (nullable = true)\n",
      " |-- _c19: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Read CSV File\n",
    "df = spark.read.option(\"delimiter\", \";\") \\\n",
    "                        .option(\"header\",True) \\\n",
    "                        .csv(\"datasets/brazil-weather/conventional_weather_stations_inmet_brazil_1961_2019.csv\", )\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tambien existe una opcion para inferir los tipos de datos de las columnas, aunque esto requiere leer una segunda vez el conjunto de datos. Esto ultimo es algo a considerar cuando trabajamos con datasets muy grandes y cada operacion es costosa. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Estacao: integer (nullable = true)\n",
      " |-- Data: string (nullable = true)\n",
      " |-- Hora: integer (nullable = true)\n",
      " |-- Precipitacao: double (nullable = true)\n",
      " |-- TempBulboSeco: double (nullable = true)\n",
      " |-- TempBulboUmido: double (nullable = true)\n",
      " |-- TempMaxima: double (nullable = true)\n",
      " |-- TempMinima: double (nullable = true)\n",
      " |-- UmidadeRelativa: double (nullable = true)\n",
      " |-- PressaoAtmEstacao: double (nullable = true)\n",
      " |-- PressaoAtmMar: double (nullable = true)\n",
      " |-- DirecaoVento: double (nullable = true)\n",
      " |-- VelocidadeVento: double (nullable = true)\n",
      " |-- Insolacao: double (nullable = true)\n",
      " |-- Nebulosidade: double (nullable = true)\n",
      " |-- Evaporacao Piche: double (nullable = true)\n",
      " |-- Temp Comp Media: double (nullable = true)\n",
      " |-- Umidade Relativa Media: double (nullable = true)\n",
      " |-- Velocidade do Vento Media: double (nullable = true)\n",
      " |-- _c19: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pasamos las opciones al objeto DataFrameReader usando 'options' en lugar de 'option' para llamar una vez al metodo. Solo por ser una manera alternativa de llamarlo. No significa un costo menor. \n",
    "df = spark.read.options(delimiter=\";\", header=True, inferSchema=True) \\\n",
    "                        .csv(\"datasets/brazil-weather/conventional_weather_stations_inmet_brazil_1961_2019.csv\", )\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Estacao: integer (nullable = true)\n",
      " |-- Data: string (nullable = true)\n",
      " |-- Hora: integer (nullable = true)\n",
      " |-- Precipitacao: double (nullable = true)\n",
      " |-- TempBulboSeco: double (nullable = true)\n",
      " |-- TempBulboUmido: double (nullable = true)\n",
      " |-- TempMaxima: double (nullable = true)\n",
      " |-- TempMinima: double (nullable = true)\n",
      " |-- UmidadeRelativa: double (nullable = true)\n",
      " |-- PressaoAtmEstacao: double (nullable = true)\n",
      " |-- PressaoAtmMar: double (nullable = true)\n",
      " |-- DirecaoVento: double (nullable = true)\n",
      " |-- VelocidadeVento: double (nullable = true)\n",
      " |-- Insolacao: double (nullable = true)\n",
      " |-- Nebulosidade: double (nullable = true)\n",
      " |-- Evaporacao Piche: double (nullable = true)\n",
      " |-- Temp Comp Media: double (nullable = true)\n",
      " |-- Umidade Relativa Media: double (nullable = true)\n",
      " |-- Velocidade do Vento Media: double (nullable = true)\n",
      " |-- _c19: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# tambien se pueden especificar las opciones en un diccionario y pasar el parametro como **kwargs\n",
    "\n",
    "# Define read options\n",
    "options = {\n",
    "    \"inferSchema\": \"True\",\n",
    "    \"delimiter\": \";\",\n",
    "    \"header\":True\n",
    "}\n",
    "\n",
    "# Read a CSV file with specified options\n",
    "df = spark.read.options(**options).csv(\"datasets/brazil-weather/conventional_weather_stations_inmet_brazil_1961_2019.csv\")\n",
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformaciones de DataFrame\n",
    "Las transformaciones de DataFrame en PySpark implican la aplicación de varias operaciones para manipular los datos dentro de un DataFrame. Estas transformaciones incluyen:\n",
    "\n",
    "- Filtros: Seleccionar filas del DataFrame basadas en ciertas condiciones.\n",
    "- Selección de Columnas: Extraer columnas específicas del DataFrame.\n",
    "- Añadir Columnas: Crear nuevas columnas realizando cálculos o transformaciones en las columnas existentes.\n",
    "- Eliminar Columnas: Eliminar columnas innecesarias del DataFrame.\n",
    "- Agrupamiento y Agregación: Agrupar filas según ciertos criterios y calcular estadísticas agregadas, como suma, promedio, conteo, etc., dentro de cada grupo.\n",
    "- Orden: Organizar las filas del DataFrame en un orden especificado basado en los valores de las columnas.\n",
    "- Unión: Combinar dos DataFrames basándose en una clave o condición común.\n",
    "- Unión Vertical: Concatenar dos DataFrames verticalmente, añadiendo filas de un DataFrame a otro.\n",
    "- Pivoting & Melting: Reestructurar el DataFrame de formato largo a formato ancho (pivot) o de formato ancho a formato largo (melt).\n",
    "- Funciones de ventana deslizante: Realizar cálculos sobre una ventana deslizante de filas, como calcular promedios móviles o rankings.\n",
    "\n",
    "Es notable que todas estas funciones se pueden hacer con pandas y python. La necesidad aqui de usarlas en el contexto de PySpark es la posibilidad de trabajar estos metodos sobre RDDs y obteniendo un mejor rendimiento con datasets grandes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejemplos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Operaciones de transformacion: `when`\n",
    "\n",
    "Al igual que en SQL y en otros lenguajes de programación, PySpark admite una forma de verificar múltiples condiciones en secuencia y devuelve un valor cuando se cumple la primera condición, utilizando expresiones como case when en SQL y `when().otherwise()` en PySpark. Estas funcionan de manera similar a las declaraciones `switch` y `if then else`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+----+------------+-------------+--------------+----------+----------+---------------+-----------------+-------------+------------+---------------+---------+------------+----------------+---------------+----------------------+-------------------------+----+\n",
      "|Estacao|      Data|Hora|Precipitacao|TempBulboSeco|TempBulboUmido|TempMaxima|TempMinima|UmidadeRelativa|PressaoAtmEstacao|PressaoAtmMar|DirecaoVento|VelocidadeVento|Insolacao|Nebulosidade|Evaporacao Piche|Temp Comp Media|Umidade Relativa Media|Velocidade do Vento Media|_c19|\n",
      "+-------+----------+----+------------+-------------+--------------+----------+----------+---------------+-----------------+-------------+------------+---------------+---------+------------+----------------+---------------+----------------------+-------------------------+----+\n",
      "|  82024|01/01/1961|   0|        NULL|         NULL|          NULL|      32.3|      NULL|           NULL|             NULL|         NULL|        NULL|           NULL|      4.4|        NULL|            NULL|          26.56|                  82.5|                      3.0|NULL|\n",
      "|  82024|01/01/1961|1200|        NULL|         Alta|          23.9|      NULL|      22.9|           83.0|            994.2|         NULL|         5.0|            5.0|     NULL|         8.0|            NULL|           NULL|                  NULL|                     NULL|NULL|\n",
      "|  82024|01/01/1961|1800|        NULL|         Alta|          27.0|      NULL|      NULL|           65.0|            991.6|         NULL|         5.0|            3.0|     NULL|         9.0|            NULL|           NULL|                  NULL|                     NULL|NULL|\n",
      "|  82024|02/01/1961|   0|        NULL|         Alta|          24.6|      33.2|      NULL|           91.0|            991.9|         NULL|         9.0|            1.0|     10.0|         7.0|            NULL|          28.06|                  77.5|                 5.666667|NULL|\n",
      "|  82024|02/01/1961|1200|        16.0|         Alta|          24.0|      NULL|      23.7|           78.0|            995.0|         NULL|         5.0|            7.0|     NULL|         7.0|            NULL|           NULL|                  NULL|                     NULL|NULL|\n",
      "|  82024|02/01/1961|1800|        NULL|         Alta|          27.2|      NULL|      NULL|           64.0|            993.4|         NULL|         5.0|            7.0|     NULL|         8.0|            NULL|           NULL|                  NULL|                     NULL|NULL|\n",
      "|  82024|03/01/1961|   0|        NULL|         Alta|          26.2|      32.9|      NULL|           84.0|            992.3|         NULL|         5.0|            3.0|      7.5|         5.0|             8.0|          28.08|                 70.75|                 6.333333|NULL|\n",
      "|  82024|03/01/1961|1200|         0.0|         Alta|          23.9|      NULL|      23.3|           76.0|            995.6|         NULL|         9.0|            7.0|     NULL|         7.0|            NULL|           NULL|                  NULL|                     NULL|NULL|\n",
      "|  82024|03/01/1961|1800|        NULL|         Alta|          25.7|      NULL|      NULL|           57.0|            993.9|         NULL|         5.0|            9.0|     NULL|         8.0|            NULL|           NULL|                  NULL|                     NULL|NULL|\n",
      "|  82024|04/01/1961|   0|        NULL|         Alta|          25.3|      30.4|      NULL|           75.0|            991.2|         NULL|         5.0|            3.0|      5.2|         5.0|             3.2|          27.02|                 78.75|                      7.0|NULL|\n",
      "|  82024|04/01/1961|1200|         0.0|         Alta|          24.5|      NULL|      23.8|           84.0|            994.4|         NULL|         9.0|            7.0|     NULL|         7.0|            NULL|           NULL|                  NULL|                     NULL|NULL|\n",
      "|  82024|04/01/1961|1800|        NULL|         Alta|          26.0|      NULL|      NULL|           73.0|            993.9|         NULL|         5.0|            9.0|     NULL|         9.0|            NULL|           NULL|                  NULL|                     NULL|NULL|\n",
      "|  82024|05/01/1961|   0|        NULL|         Alta|          24.4|      32.9|      NULL|           79.0|            993.4|         NULL|         5.0|            5.0|      9.5|         5.0|             7.4|          28.14|                  73.0|                 6.333333|NULL|\n",
      "|  82024|05/01/1961|1200|         0.0|         Alta|          24.7|      NULL|      23.1|           81.0|            994.0|         NULL|         5.0|            9.0|     NULL|         6.0|            NULL|           NULL|                  NULL|                     NULL|NULL|\n",
      "|  82024|05/01/1961|1800|        NULL|         Alta|          26.6|      NULL|      NULL|           65.0|            992.6|         NULL|         5.0|            7.0|     NULL|         7.0|            NULL|           NULL|                  NULL|                     NULL|NULL|\n",
      "|  82024|06/01/1961|   0|        NULL|         Alta|          25.1|      32.4|      NULL|           73.0|            990.8|         NULL|         5.0|            3.0|      8.1|         5.0|             7.2|          28.24|                 75.75|                      5.0|NULL|\n",
      "|  82024|06/01/1961|1200|         0.0|         Alta|          24.8|      NULL|      24.0|           83.0|            994.0|         NULL|         5.0|            7.0|     NULL|         7.0|            NULL|           NULL|                  NULL|                     NULL|NULL|\n",
      "|  82024|06/01/1961|1800|        NULL|         Alta|          27.1|      NULL|      NULL|           66.0|            991.0|         NULL|         9.0|            5.0|     NULL|         8.0|            NULL|           NULL|                  NULL|                     NULL|NULL|\n",
      "|  82024|07/01/1961|   0|        NULL|         Alta|          25.8|      31.2|      NULL|           77.0|            989.9|         NULL|         5.0|            3.0|      2.4|         6.0|             4.7|          27.32|                 81.75|                 4.333333|NULL|\n",
      "|  82024|07/01/1961|1200|         0.0|         Alta|          24.2|      NULL|      23.3|           85.0|            994.3|         NULL|         5.0|            3.0|     NULL|         7.0|            NULL|           NULL|                  NULL|                     NULL|NULL|\n",
      "+-------+----------+----+------------+-------------+--------------+----------+----------+---------------+-----------------+-------------+------------+---------------+---------+------------+----------------+---------------+----------------------+-------------------------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when\n",
    "df2 = df.withColumn(\"TempBulboSeco\", when(df.TempBulboSeco <= 10,\"Baja\") \\\n",
    "                                 .when(df.TempBulboSeco <= 20,\"Moderada\") \\\n",
    "                                 .when(df.TempBulboSeco > 20,\"Alta\"))\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operaciones de filtro: `filter` y `where`\n",
    "\n",
    "Ahora filtremos el dataset para visualizar solo los datos de las estaciones etiquetados. Dejamos fuera los nulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+-------------+--------------+\n",
      "|Estacao|      Data|TempBulboSeco|TempBulboUmido|\n",
      "+-------+----------+-------------+--------------+\n",
      "|  82024|21/08/1976|         Baja|          22.8|\n",
      "|  82191|01/05/1970|         Baja|          25.5|\n",
      "|  82331|20/05/1992|         Baja|           0.0|\n",
      "|  82331|20/05/1992|         Baja|           6.5|\n",
      "|  82331|21/05/1992|         Baja|           0.0|\n",
      "|  83182|01/11/1996|         Baja|           0.0|\n",
      "|  83182|01/11/1996|         Baja|           0.0|\n",
      "|  83182|02/11/1996|         Baja|           0.0|\n",
      "|  83264|18/07/1975|         Baja|           3.1|\n",
      "|  83264|19/07/1981|         Baja|           5.8|\n",
      "|  83264|09/06/1985|         Baja|           8.0|\n",
      "|  83264|01/04/1986|         Baja|          23.0|\n",
      "|  83264|07/07/1989|         Baja|           8.6|\n",
      "|  83267|15/08/1999|         Baja|           5.9|\n",
      "|  83267|13/07/2000|         Baja|           8.7|\n",
      "|  83267|14/07/2000|         Baja|           7.7|\n",
      "|  83270|18/05/1998|         Baja|          20.6|\n",
      "|  83270|20/02/1999|         Baja|          23.9|\n",
      "|  83586|18/05/1968|         Baja|           8.6|\n",
      "|  83586|06/06/1969|         Baja|           7.3|\n",
      "+-------+----------+-------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Using equal condition\n",
    "df2['Estacao', 'Data', 'TempBulboSeco', 'TempBulboUmido'] \\\n",
    "                                .filter(df2.TempBulboSeco.isNotNull()) \\\n",
    "                                .filter(df2.TempBulboUmido.isNotNull()) \\\n",
    "                                .filter(df2.TempBulboSeco == 'Baja').show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio\n",
    "\n",
    "Al transformar la columna `TempBulboSeco` estamos reduciendo y por lo tanto perdiendo informacion de la columna, pasando de un dato double a una categoria en string. Si bien esto puede ser deseable para agrupar, no hay razones por ahora para perder la columna original. En este ejercicio, van a crear una columna `TempBulboSeco_cat` que contenga la transformacion hecha mas arriba, y que conserve esa categoria, sin eliminar ni perder la columna original `TempBulboSeco`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `groupBy`, `filter` y `sort`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a hacer un ejemplo de agrupacion, filtrado y ordenado sobre nuestro conjunto de datos. Supongamos que necesitamos saber la temperatura media de bulbo seco, por estacion. Podriamos hacer lo siguiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|Estacao| TempBulboSeco_avg|\n",
      "+-------+------------------+\n",
      "|  82690|30.127860210855143|\n",
      "|  82296|29.659932405827835|\n",
      "|  82474| 29.30886615176194|\n",
      "|  82678|29.267836075752573|\n",
      "|  82879|29.260869697894222|\n",
      "|  82460|29.218999332229416|\n",
      "|  82298|29.210521264272458|\n",
      "|  82780| 29.11393250585912|\n",
      "|  82975|29.087137417260166|\n",
      "|  82870|29.035611291845285|\n",
      "|  82480|28.966630503571423|\n",
      "|  82791|28.950074794315977|\n",
      "|  82476|  28.9276052987726|\n",
      "|  82590|28.869825836093444|\n",
      "|  82578|28.858674607196377|\n",
      "|  82493| 28.84008219335511|\n",
      "|  82693|28.707614601018964|\n",
      "|  82588| 28.68681230116714|\n",
      "|  82564| 28.56894557892424|\n",
      "|  82392|28.565107830433384|\n",
      "+-------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg, col, desc\n",
    "# Hacemos un promedio por estacion de la temperatura de bulbo seco\n",
    "df.groupBy(\"Estacao\") \\\n",
    "  .agg(avg(\"TempBulboSeco\").alias(\"TempBulboSeco_avg\")) \\\n",
    "  .filter(col(\"TempBulboSeco_avg\") > 10)  \\\n",
    "  .sort(desc(\"TempBulboSeco_avg\")) \\\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|Estacao| TempBulboSeco_avg|\n",
      "+-------+------------------+\n",
      "|  82690|30.127860210855143|\n",
      "|  82296|29.659932405827835|\n",
      "|  82474| 29.30886615176194|\n",
      "|  82678|29.267836075752573|\n",
      "|  82879|29.260869697894222|\n",
      "|  82460|29.218999332229416|\n",
      "|  82298|29.210521264272458|\n",
      "|  82780| 29.11393250585912|\n",
      "|  82975|29.087137417260166|\n",
      "|  82870|29.035611291845285|\n",
      "|  82480|28.966630503571423|\n",
      "|  82791|28.950074794315977|\n",
      "|  82476|  28.9276052987726|\n",
      "|  82590|28.869825836093444|\n",
      "|  82578|28.858674607196377|\n",
      "|  82493| 28.84008219335511|\n",
      "|  82693|28.707614601018964|\n",
      "|  82588| 28.68681230116714|\n",
      "|  82564| 28.56894557892424|\n",
      "|  82392|28.565107830433384|\n",
      "+-------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Mismo ejemplo usando Spark SQL\n",
    "df.createOrReplaceTempView(\"EMP\")\n",
    "spark.sql(\"select Estacao, avg(TempBulboSeco) as TempBulboSeco_avg from EMP \" +\n",
    "          \"group by Estacao having TempBulboSeco_avg > 10 \" + \n",
    "          \"order by TempBulboSeco_avg desc\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio\n",
    "\n",
    "Cuantos casos hay por cada estacion, para cada una de las categorias creadas en `TempBulboSeco_cat`?\n",
    "\n",
    "Escribir un filtro usando las funciones de spark, o spark SQL."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
