{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "653d04bc-8cbd-4b79-b2b5-3023ec87265b",
   "metadata": {
    "tags": []
   },
   "source": [
    "Para implementar las funciones map y reduce en Python sin usar las funciones nativas y siguiendo la arquitectura propuesta en el paper de Jeffrey Dean sobre MapReduce, vamos a enfocarnos en el ejemplo clásico de contar palabras. Aquí te explicaré el enfoque y luego procederemos a escribir el código.\n",
    "\n",
    "Enfoque\n",
    "Map Function: Esta función tomará un documento o un conjunto de documentos y dividirá el texto en palabras. Para cada palabra, generará un par (clave, valor), donde la clave es la palabra y el valor es 1, indicando que la palabra ha aparecido una vez.\n",
    "\n",
    "Shuffle and Sort: Después de la fase de map, los pares clave-valor necesitan ser ordenados y agrupados por clave. Esto facilitará el proceso de reducción agrupando todas las ocurrencias de una palabra específica.\n",
    "\n",
    "Reduce Function: Esta función tomará los pares clave-valor agrupados por clave y sumará los valores para cada clave. Esto efectivamente contará la cantidad total de veces que cada palabra aparece en el documento o conjunto de documentos.\n",
    "\n",
    "Integración: Integraremos estas funciones en un sistema que pueda tomar un conjunto de documentos como entrada, aplicar las funciones map y reduce, y producir un conteo final de palabras.\n",
    "\n",
    "Implementación\n",
    "Ahora procederé a escribir el código en el Python Code Interpreter (PCI) siguiendo este enfoque. Después de eso, revisaremos y optimizaremos el código juntos.\n",
    "\n",
    "He implementado las funciones map, shuffle_and_sort, reduce, y una función integradora map_reduce para contar palabras en un conjunto de documentos, siguiendo la arquitectura de MapReduce. Aquí hay un resumen del código:\n",
    "\n",
    "map_function: Esta función toma una lista de documentos y para cada palabra en un documento, produce un par (palabra, 1).\n",
    "\n",
    "shuffle_and_sort: Después de la fase de map, esta función ordena y agrupa los valores por clave (palabra), preparándolos para la reducción.\n",
    "\n",
    "reduce_function: Toma los valores agrupados por clave de la fase de shuffle and sort y suma los valores para cada clave, contando así las ocurrencias totales de cada palabra.\n",
    "\n",
    "map_reduce: Esta es la función integradora que orquesta el proceso de MapReduce aplicando las funciones de map, shuffle and sort, y reduce para contar palabras en los documentos proporcionados.\n",
    "\n",
    "El siguiente paso es revisar y optimizar este código. Específicamente, podemos enfocarnos en la eficiencia de estas funciones, la claridad del código, y asegurarnos de que sigue las buenas prácticas de Python. ¿Te gustaría hacer alguna modificación o añadir alguna característica específica a esta implementación? ​"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "bea9cbf9-6335-4b7d-bbf0-6b5265d32ec5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Implementación de Map y Reduce sin usar funciones nativas\n",
    "\n",
    "def map_function(documents):\n",
    "    \"\"\"\n",
    "    Map function that processes a list of documents and returns a list of tuples (word, 1)\n",
    "    for each word in the document.\n",
    "    \"\"\"\n",
    "    word_counts = []\n",
    "    for document in documents.values():\n",
    "        # Dividir el documento en palabras\n",
    "        words = document.split()\n",
    "        for word in words:\n",
    "            word_counts.append((word, 1))\n",
    "    return word_counts\n",
    "\n",
    "def shuffle_and_sort(mapped_values):\n",
    "    \"\"\"\n",
    "    Shuffle and sort function that takes the output of the map function, sorts it,\n",
    "    and groups values by key (word).\n",
    "    \"\"\"\n",
    "    sorted_values = sorted(mapped_values, key=lambda x: x[0])\n",
    "    grouped_values = {}\n",
    "    for key, value in sorted_values:\n",
    "        if key in grouped_values:\n",
    "            grouped_values[key].append(value)\n",
    "        else:\n",
    "            grouped_values[key] = [value]\n",
    "    return grouped_values\n",
    "\n",
    "def reduce_function(grouped_values):\n",
    "    \"\"\"\n",
    "    Reduce function that processes grouped values and sums the counts for each word.\n",
    "    \"\"\"\n",
    "    reduced_values = {}\n",
    "    for key, values in grouped_values.items():\n",
    "        reduced_values[key] = sum(values)\n",
    "    return reduced_values\n",
    "\n",
    "def map_reduce(documents):\n",
    "    \"\"\"\n",
    "    Integrates map, shuffle_and_sort, and reduce functions to count words in documents.\n",
    "    \"\"\"\n",
    "    mapped_values = map_function(documents)\n",
    "    grouped_values = shuffle_and_sort(mapped_values)\n",
    "    reduced_values = reduce_function(grouped_values)\n",
    "    return reduced_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8ed1fd17-61df-403e-9549-4daf2a2d0873",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "documents = { \n",
    "    \"historia_corta\": \"El lugar es oscuro, pero igual sigo caminando. La clase esta buena, pero igual sigo faltando. El dia esta lindo, no esta para quedarse encerrado. Ir en auto tarda menos, ir caminando tarda mas.\" ,\n",
    "    \"palabras\": \"El domador de focas esta pensando en renunciar.\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "880a3d4f-3428-4f77-ab87-682511733b51",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'El': 3,\n",
       " 'Ir': 1,\n",
       " 'La': 1,\n",
       " 'auto': 1,\n",
       " 'buena,': 1,\n",
       " 'caminando': 1,\n",
       " 'caminando.': 1,\n",
       " 'clase': 1,\n",
       " 'de': 1,\n",
       " 'dia': 1,\n",
       " 'domador': 1,\n",
       " 'en': 2,\n",
       " 'encerrado.': 1,\n",
       " 'es': 1,\n",
       " 'esta': 4,\n",
       " 'faltando.': 1,\n",
       " 'focas': 1,\n",
       " 'igual': 2,\n",
       " 'ir': 1,\n",
       " 'lindo,': 1,\n",
       " 'lugar': 1,\n",
       " 'mas.': 1,\n",
       " 'menos,': 1,\n",
       " 'no': 1,\n",
       " 'oscuro,': 1,\n",
       " 'para': 1,\n",
       " 'pensando': 1,\n",
       " 'pero': 2,\n",
       " 'quedarse': 1,\n",
       " 'renunciar.': 1,\n",
       " 'sigo': 2,\n",
       " 'tarda': 2}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map_reduce(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928aa4bf-019a-4a91-a706-2b8fbb7146d2",
   "metadata": {},
   "source": [
    "Para implementar una versión didáctica de las funciones de map y reduce utilizando las funciones nativas de Python map, lambda, y reduce (esta última disponible en el módulo functools), centrémonos de nuevo en el ejemplo del contador de palabras. Esta versión ilustrará cómo Python puede simplificar significativamente la implementación de la funcionalidad MapReduce con sus características incorporadas.\n",
    "\n",
    "Enfoque Didáctico\n",
    "Map Phase: Usaremos la función nativa map con una función lambda para procesar cada documento, dividiéndolo en palabras y generando pares (palabra, 1) para cada palabra.\n",
    "\n",
    "Shuffle and Sort Phase: Aprovecharemos las capacidades de agrupación de Python, específicamente con la función groupby de itertools, después de ordenar los pares por clave (palabra) para agrupar los resultados del map.\n",
    "\n",
    "Reduce Phase: Utilizaremos reduce de functools junto con funciones lambda para sumar las ocurrencias de cada palabra.\n",
    "\n",
    "Integración: Combinaremos estas fases en una función de alto nivel que ejecute el proceso completo de MapReduce para contar palabras.\n",
    "\n",
    "Ahora, voy a implementar esta versión.\n",
    "\n",
    "He creado una versión didáctica utilizando las funciones nativas map, lambda, y reduce de Python para implementar el contador de palabras siguiendo el modelo MapReduce. Aquí está el desglose del código:\n",
    "\n",
    "map_phase: Esta función utiliza map y una expresión lambda para procesar cada documento en pares (palabra, 1).\n",
    "\n",
    "reduce_phase: Aquí combinamos reduce de functools y groupby de itertools para agrupar y sumar las ocurrencias de cada palabra. Primero, aplanamos la lista de listas de pares generados en la fase de map, los ordenamos por palabra, y luego agrupamos por palabra. Para cada grupo, usamos reduce para sumar las ocurrencias.\n",
    "\n",
    "map_reduce_native: Esta función integra las fases de map y reduce para contar las palabras en los documentos dados, mostrando el poder de las funciones de alto orden de Python para realizar tareas de procesamiento de datos complejas de manera concisa y eficiente.\n",
    "\n",
    "Este enfoque muestra cómo las funciones nativas de Python pueden ser extremadamente útiles para implementar patrones de diseño complejos, como MapReduce, de una manera mucho más directa y legible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4dacc68c-6919-49d0-ba28-0a16da91359a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "from itertools import groupby\n",
    "from operator import itemgetter\n",
    "\n",
    "def map_phase(documents):\n",
    "    \"\"\"\n",
    "    Map phase using Python's native map function. It processes each document into (word, 1) pairs.\n",
    "    \"\"\"\n",
    "    return list(map(lambda doc: [(word, 1) for word in doc.split()], documents.values()))\n",
    "\n",
    "def reduce_phase(mapped_values):\n",
    "    \"\"\"\n",
    "    Reduce phase that sums up the occurrences of each word using Python's native reduce function.\n",
    "    \"\"\"\n",
    "    # Flatten the list of lists\n",
    "    all_items = [item for sublist in mapped_values for item in sublist]\n",
    "    # Sort items by word for grouping\n",
    "    sorted_items = sorted(all_items, key=itemgetter(0))\n",
    "    # Group by word and reduce\n",
    "    grouped_items = groupby(sorted_items, key=itemgetter(0))\n",
    "    reduced_items = {key: reduce(lambda x, y: x + y[1], group, 0) for key, group in grouped_items}\n",
    "    return reduced_items\n",
    "\n",
    "def map_reduce_native(documents):\n",
    "    \"\"\"\n",
    "    Integrates map and reduce phases to count words in documents using native Python functions.\n",
    "    \"\"\"\n",
    "    mapped_values = map_phase(documents)\n",
    "    reduced_values = reduce_phase(mapped_values)\n",
    "    return reduced_values\n",
    "\n",
    "# Comentar las llamadas a funciones para la revisión de código\n",
    "# documents = [\"hello world\", \"world of mapreduce\", \"hello mapreduce world\"]\n",
    "# word_counts = map_reduce_native(documents)\n",
    "# print(word_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "aed81f86-1b71-46ab-94ec-6d2142b306c0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'El': 3,\n",
       " 'Ir': 1,\n",
       " 'La': 1,\n",
       " 'auto': 1,\n",
       " 'buena,': 1,\n",
       " 'caminando': 1,\n",
       " 'caminando.': 1,\n",
       " 'clase': 1,\n",
       " 'de': 1,\n",
       " 'dia': 1,\n",
       " 'domador': 1,\n",
       " 'en': 2,\n",
       " 'encerrado.': 1,\n",
       " 'es': 1,\n",
       " 'esta': 4,\n",
       " 'faltando.': 1,\n",
       " 'focas': 1,\n",
       " 'igual': 2,\n",
       " 'ir': 1,\n",
       " 'lindo,': 1,\n",
       " 'lugar': 1,\n",
       " 'mas.': 1,\n",
       " 'menos,': 1,\n",
       " 'no': 1,\n",
       " 'oscuro,': 1,\n",
       " 'para': 1,\n",
       " 'pensando': 1,\n",
       " 'pero': 2,\n",
       " 'quedarse': 1,\n",
       " 'renunciar.': 1,\n",
       " 'sigo': 2,\n",
       " 'tarda': 2}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map_reduce_native(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c292588-51f0-4bf6-9edc-43e905906caf",
   "metadata": {},
   "source": [
    "Crear una versión del contador de palabras utilizando PySpark implica aprovechar el modelo de programación distribuida que Spark ofrece, lo cual es especialmente útil para procesar grandes volúmenes de datos. Aquí, las operaciones de map y reduce se realizan a través de transformaciones y acciones en RDDs (Resilient Distributed Datasets) o DataFrames, dependiendo del enfoque que prefieras. Para este ejemplo, usaré RDDs por su cercanía al modelo MapReduce.\n",
    "\n",
    "Enfoque PySpark\n",
    "Crear RDD: Inicialmente, los documentos se cargarán en un RDD. Cada documento puede ser una línea de un archivo o un elemento en una lista si cargamos los datos directamente para fines didácticos.\n",
    "\n",
    "Fase de Map: Aplicaremos una transformación flatMap para dividir los documentos en palabras y mapear cada palabra a un par (palabra, 1).\n",
    "\n",
    "Fase de Reduce: Utilizaremos la acción reduceByKey para sumar todas las ocurrencias de cada palabra.\n",
    "\n",
    "Acción de Recolección: Finalmente, con la acción collect, recogeremos los resultados del conteo de palabras.\n",
    "\n",
    "Implementación en PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "44ab4125-e912-42f1-9d7f-cb5c1d67057e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "def count_words_with_pyspark(documents):\n",
    "    # Inicializar SparkContext\n",
    "    sc = SparkContext(\"local\", \"WordCount\")\n",
    "    \n",
    "    # Cargar documentos en un RDD\n",
    "    rdd = sc.parallelize(documents.values())\n",
    "    \n",
    "    # Fase de Map: dividir en palabras y mapear a (palabra, 1)\n",
    "    mapped_rdd = rdd.flatMap(lambda document: document.split()).map(lambda word: (word, 1))\n",
    "    \n",
    "    # Fase de Reduce: sumar las ocurrencias de cada palabra\n",
    "    reduced_rdd = mapped_rdd.reduceByKey(lambda a, b: a + b)\n",
    "    \n",
    "    # Recolectar y devolver los resultados\n",
    "    word_counts = reduced_rdd.collect()\n",
    "    \n",
    "    # Detener el SparkContext\n",
    "    sc.stop()\n",
    "    \n",
    "    return word_counts\n",
    "\n",
    "# Ejemplo de uso\n",
    "# documents = [\"hello world\", \"world of mapreduce\", \"hello mapreduce world\"]\n",
    "# word_counts = count_words_with_pyspark(documents)\n",
    "# print(word_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d3606b13-4781-4bca-a3b3-3e54defc1ab1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('El', 3),\n",
       " ('lugar', 1),\n",
       " ('es', 1),\n",
       " ('oscuro,', 1),\n",
       " ('pero', 2),\n",
       " ('igual', 2),\n",
       " ('sigo', 2),\n",
       " ('caminando.', 1),\n",
       " ('La', 1),\n",
       " ('clase', 1),\n",
       " ('esta', 4),\n",
       " ('buena,', 1),\n",
       " ('faltando.', 1),\n",
       " ('dia', 1),\n",
       " ('lindo,', 1),\n",
       " ('no', 1),\n",
       " ('para', 1),\n",
       " ('quedarse', 1),\n",
       " ('encerrado.', 1),\n",
       " ('Ir', 1),\n",
       " ('en', 2),\n",
       " ('auto', 1),\n",
       " ('tarda', 2),\n",
       " ('menos,', 1),\n",
       " ('ir', 1),\n",
       " ('caminando', 1),\n",
       " ('mas.', 1),\n",
       " ('domador', 1),\n",
       " ('de', 1),\n",
       " ('focas', 1),\n",
       " ('pensando', 1),\n",
       " ('renunciar.', 1)]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_words_with_pyspark(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0969027-103f-4416-a5e0-781d6f6a4d05",
   "metadata": {},
   "source": [
    "Este código debe ejecutarse en un entorno que tenga PySpark configurado y listo para usar, ya que PySpark no está disponible en este entorno de código interactivo. La función count_words_with_pyspark demuestra cómo cargar datos, aplicar las transformaciones de map y reduce, y recoger los resultados en un programa Spark.\n",
    "\n",
    "La implementación ilustra un enfoque típico de Spark para el problema del conteo de palabras, aprovechando la distribución y paralelización para procesar grandes conjuntos de datos eficientemente.\n",
    "\n",
    "\n",
    "Porque no usamos shuffle and sort?\n",
    "\n",
    "En realidad, Spark sí implementa las operaciones de \"shuffle and sort\" (barajado y ordenamiento), pero lo hace de manera interna y optimizada, por lo que no es necesario que el usuario las gestione explícitamente en la mayoría de los casos. Estas operaciones son una parte crucial del proceso de computación distribuida, especialmente para operaciones que involucran agrupación o reducción de datos a través de múltiples particiones, como reduceByKey, groupBy, join, entre otros.\n",
    "\n",
    "Shuffle\n",
    "El \"shuffle\" ocurre cuando los datos deben ser redistribuidos entre diferentes particiones para que las operaciones de agrupación o reducción puedan ejecutarse. Esto generalmente implica la transferencia de datos a través de la red entre los nodos del cluster, lo cual puede ser costoso en términos de rendimiento. Spark intenta minimizar el costo de esta operación con técnicas como:\n",
    "\n",
    "Pipelining: Cuando es posible, Spark realiza operaciones de map y reduce en el mismo nodo para reducir la necesidad de transferir datos.\n",
    "Optimización del Plan de Ejecución: Spark's Catalyst Optimizer puede reorganizar y combinar operaciones para minimizar el shuffle.\n",
    "Sort\n",
    "El \"sort\" (ordenamiento) es parte de ciertas operaciones que requieren que los datos estén en un orden específico. En muchas operaciones de shuffle, Spark realiza un ordenamiento de los datos para facilitar la agrupación o reducción posterior. Por ejemplo, reduceByKey primero agrupará todas las claves iguales juntas (lo cual implica un ordenamiento implícito por clave) antes de aplicar la función de reducción.\n",
    "\n",
    "Por qué no es explícito\n",
    "Abstracción y simplicidad: Uno de los objetivos de Spark es proporcionar una abstracción de alto nivel para el procesamiento de datos distribuidos, escondiendo la complejidad de las operaciones de bajo nivel como shuffle y sort. Esto hace que Spark sea más accesible y reduce el esfuerzo necesario para optimizar las aplicaciones de procesamiento de datos.\n",
    "\n",
    "Optimización interna: Spark maneja internamente las optimizaciones relacionadas con el shuffle y el sort. Tiene algoritmos y estrategias para decidir cuándo y cómo realizar estas operaciones de la manera más eficiente posible, basándose en el contexto de la aplicación y la configuración del cluster.\n",
    "\n",
    "En resumen, aunque no se gestione explícitamente en la mayoría de los casos, el \"shuffle and sort\" es una parte integral de cómo Spark ejecuta operaciones distribuidas, gestionando de forma inteligente y optimizada para mejorar el rendimiento y la eficiencia del procesamiento de datos a gran escala."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
